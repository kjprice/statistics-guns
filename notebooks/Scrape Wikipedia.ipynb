{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_URL = 'https://en.wikipedia.org/wiki/List_of_school_shootings_in_the_United_States'\n",
    "\n",
    "INFO_TO_SCRAPE = [\n",
    "    {\n",
    "        'url': 'https://en.wikipedia.org/wiki/List_of_school_shootings_in_the_United_States',\n",
    "        'name': '20th_century_school_shootings',\n",
    "        'tables_range': [0,4]\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://en.wikipedia.org/wiki/List_of_school_shootings_in_the_United_States_(before_2000)#20th_century',\n",
    "        'name': 'before_20th_century_shootings',\n",
    "        'tables_range': [0,16]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull all html from site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_from_site(site):\n",
    "    text = requests.get(site).text\n",
    "    print('found {} characters from site'.format(len(text)))\n",
    "    return text\n",
    "\n",
    "# text = get_html_from_site(INFO_TO_SCRAPE[0]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_from_html(html_string):\n",
    "    soup = BeautifulSoup(html_string,'lxml')\n",
    "    # print(soup.prettify())\n",
    "    print('found {} lines of html from site'.format(len(soup.prettify().split('\\n'))))\n",
    "    return soup\n",
    "    \n",
    "# soup = get_soup_from_html(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tables_from_wiki_soup(soup):\n",
    "    tables_found = soup.find_all('table',{'class':'sortable wikitable'})\n",
    "    print('found {} tables'.format(len(tables_found)))\n",
    "    return tables_found\n",
    "# all_tables = get_all_tables_from_wiki_soup(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rows_in_table(table):\n",
    "    rows = table.find_all('tr')\n",
    "    print('found {} rows in table'.format(len(rows)))\n",
    "    return rows\n",
    "# rows = find_rows_in_table(my_tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header_text_from_rows(rows):\n",
    "    header_text = [cell.text.strip() for cell in rows[0].find_all('th')]\n",
    "    return header_text\n",
    "    \n",
    "# header_text = get_header_text_from_rows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_cells_from_rows(rows):\n",
    "    text_rows = []\n",
    "    for row in rows[1:]: # Ignore header row\n",
    "        cells = row.find_all(['th','td'])\n",
    "        cell_texts = [cell.text.strip() for cell in cells]\n",
    "        text_rows.append(cell_texts)\n",
    "    return text_rows\n",
    "\n",
    "# text_rows = get_text_cells_from_rows(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_from_url(url, tables_range):\n",
    "    text = get_html_from_site(url)\n",
    "    soup = get_soup_from_html(text)\n",
    "    all_tables = get_all_tables_from_wiki_soup(soup)\n",
    "    \n",
    "    a, b = tables_range\n",
    "\n",
    "    return all_tables[a:b]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes_from_tables(tables):\n",
    "    dfs = []\n",
    "    for table in tables:\n",
    "        print('- ', end ='')\n",
    "        rows = find_rows_in_table(table)\n",
    "        header_text = get_header_text_from_rows(rows)\n",
    "        text_rows = get_text_cells_from_rows(rows)\n",
    "\n",
    "        df = pd.DataFrame(text_rows, columns=header_text)\n",
    "        dfs.append(df)\n",
    "\n",
    "    all_dfs = pd.concat(dfs)\n",
    "    all_dfs.reset_index(inplace=True)\n",
    "\n",
    "    return all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 20th_century_school_shootings ----\n",
      "found 632040 characters from site\n",
      "found 20363 lines of html from site\n",
      "found 4 tables\n",
      "- found 65 rows in table\n",
      "- found 88 rows in table\n",
      "- found 82 rows in table\n",
      "- found 29 rows in table\n",
      "saving ../data/20th_century_school_shootings.csv\n",
      "\n",
      "---- before_20th_century_shootings ----\n",
      "found 640696 characters from site\n",
      "found 20640 lines of html from site\n",
      "found 16 tables\n",
      "- found 2 rows in table\n",
      "- found 4 rows in table\n",
      "- found 6 rows in table\n",
      "- found 8 rows in table\n",
      "- found 11 rows in table\n",
      "- found 7 rows in table\n",
      "- found 14 rows in table\n",
      "- found 19 rows in table\n",
      "- found 11 rows in table\n",
      "- found 8 rows in table\n",
      "- found 7 rows in table\n",
      "- found 17 rows in table\n",
      "- found 18 rows in table\n",
      "- found 33 rows in table\n",
      "- found 43 rows in table\n",
      "- found 66 rows in table\n",
      "saving ../data/before_20th_century_shootings.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    dfs = []\n",
    "    for info in INFO_TO_SCRAPE:\n",
    "        url = info['url']\n",
    "        name = info['name']\n",
    "        tables_range = info['tables_range']\n",
    "\n",
    "        print('---- {} ----'.format(name))\n",
    "        \n",
    "        # Cache this network-heavy piece\n",
    "        if not 'my_tables' in info:\n",
    "            tables = get_tables_from_url(url, tables_range)\n",
    "            info['my_tables'] = tables\n",
    "            \n",
    "        my_tables = info['my_tables']\n",
    "        df = get_dataframes_from_tables(my_tables)\n",
    "\n",
    "        \n",
    "        file_path = os.path.join(DATA_DIR, '{}.csv'.format(name))\n",
    "        print('saving {}'.format(file_path))\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print('')\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "\n",
    "dfs = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
